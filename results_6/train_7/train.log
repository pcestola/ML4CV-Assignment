2025-01-10 21:33:19,561 - device          : cuda:0
2025-01-10 21:33:19,561 - model           : resnet101
2025-01-10 21:33:19,561 - mlp             : 0
2025-01-10 21:33:19,561 - vae             : False
2025-01-10 21:33:19,561 - epochs          : 50
2025-01-10 21:33:19,561 - batch           : 16
2025-01-10 21:33:19,561 - lr              : 0.0001
2025-01-10 21:33:19,561 - crop            : 512
2025-01-10 21:33:19,561 - optimizer       : adam
2025-01-10 21:33:19,561 - biases          : False
2025-01-10 21:33:19,561 - focal_loss      : 0
2025-01-10 21:33:19,561 - activation      : softmax
2025-01-10 21:33:19,561 - class_weights   : False
2025-01-10 21:33:19,562 - norm_weights    : False
2025-01-10 21:33:19,562 - p               : 0.3
2025-01-10 21:33:19,562 - entropy         : 0.1
2025-01-10 21:33:19,570 - Dataset Creati
2025-01-10 21:33:19,570 - Training images: 5125
2025-01-10 21:33:19,570 - Validation images: 1031

2025-01-10 21:33:19,571 - Dataloader Creati
2025-01-10 21:33:19,571 - Training batches: 320
2025-01-10 21:33:19,571 - Validation batches: 128

2025-01-10 21:33:23,211 - Model: deeplabv3plus_resnet101

2025-01-10 21:33:23,274 - Modello caricato correttamente su cuda:0

2025-01-10 21:33:23,277 - Inizio del training
2025-01-10 21:33:23,277 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-01-10 21:33:23,277 - loss:      CE_EntropyMinimization(
  (ce_loss): CrossEntropyLoss()
)
2025-01-10 21:33:23,278 - epochs:    15
2025-01-10 21:33:23,278 - name:      weights_0

2025-01-10 21:39:52,464 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-10 21:39:52,465 - 0        1.246823   0.896982   0.257599   0.000000   0.777264   1.0e-04 , 1.0e-05 
2025-01-10 21:46:21,705 - 1        0.873259   0.801492   0.288305   0.000000   0.803065   1.0e-04 , 1.0e-05 
2025-01-10 21:46:21,706 - > saved mIoU
2025-01-10 21:52:53,073 - 2        0.791460   0.748760   0.303875   0.000000   0.812313   1.0e-04 , 1.0e-05 
2025-01-10 21:52:53,073 - > saved mIoU
2025-01-10 21:59:23,219 - 3        0.740034   0.738310   0.308457   0.000000   0.815634   1.0e-04 , 1.0e-05 
2025-01-10 21:59:23,219 - > saved mIoU
2025-01-10 22:05:52,973 - 4        0.720323   0.744923   0.318121   0.000000   0.814534   1.0e-04 , 1.0e-05 
2025-01-10 22:05:52,974 - > saved mIoU
2025-01-10 22:12:20,263 - 5        0.698016   0.765313   0.318910   0.000000   0.802607   1.0e-04 , 1.0e-05 
2025-01-10 22:12:20,263 - > saved mIoU
2025-01-10 22:18:49,525 - 6        0.675797   0.732509   0.328210   0.000000   0.817567   1.0e-04 , 1.0e-05 
2025-01-10 22:18:49,526 - > saved mIoU
2025-01-10 22:25:19,653 - 7        0.657759   0.728630   0.329869   0.000000   0.822676   1.0e-04 , 1.0e-05 
2025-01-10 22:25:19,654 - > saved mIoU
2025-01-10 22:31:48,071 - 8        0.645393   0.717907   0.338428   0.000000   0.823855   1.0e-04 , 1.0e-05 
2025-01-10 22:31:48,072 - > saved mIoU
2025-01-10 22:38:17,945 - 9        0.631323   0.731316   0.337175   0.000000   0.824917   1.0e-04 , 1.0e-05 
2025-01-10 22:44:46,825 - 10       0.622983   0.741302   0.344528   0.000000   0.818438   1.0e-04 , 1.0e-05 
2025-01-10 22:44:46,825 - > saved mIoU
2025-01-10 22:51:17,670 - 11       0.614135   0.751014   0.345803   0.000000   0.816220   1.0e-04 , 1.0e-05 
2025-01-10 22:51:17,670 - > saved mIoU
2025-01-10 22:57:45,781 - 12       0.610059   0.735371   0.350210   0.000000   0.817129   1.0e-04 , 1.0e-05 
2025-01-10 22:57:45,781 - > saved mIoU
2025-01-10 23:04:14,109 - 13       0.593262   0.740673   0.349253   0.000000   0.823713   1.0e-04 , 1.0e-05 
2025-01-10 23:10:43,961 - 14       0.597919   0.756556   0.350165   0.001033   0.815787   1.0e-04 , 1.0e-05 
2025-01-10 23:10:43,962 - Fine del training

2025-01-10 23:10:44,235 - 
Caricati i pesi in: /raid/homespace/piecestola/space/ML4CV/results/train_7/ckpts/weights_mIoU_0.pt

2025-01-10 23:10:44,237 - Inizio del training
2025-01-10 23:10:44,237 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 5e-05
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 5e-06
    maximize: False
    weight_decay: 0.0001
)
2025-01-10 23:10:44,237 - loss:      CE_EntropyMinimization(
  (ce_loss): CrossEntropyLoss()
)
2025-01-10 23:10:44,237 - epochs:    50
2025-01-10 23:10:44,237 - name:      weights_1

2025-01-10 23:26:47,810 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-10 23:26:47,810 - 0        0.494294   0.535414   0.411754   0.000000   0.877204   5.0e-05 , 5.0e-06 
2025-01-10 23:42:52,944 - 1        0.369004   0.492872   0.443855   0.000000   0.902540   5.0e-05 , 5.0e-06 
2025-01-10 23:42:52,944 - > saved mIoU
2025-01-10 23:58:58,911 - 2        0.308580   0.484053   0.455571   0.000000   0.899717   5.0e-05 , 5.0e-06 
2025-01-10 23:58:58,912 - > saved mIoU
2025-01-11 00:15:04,280 - 3        0.266557   0.465982   0.470962   0.042533   0.907899   5.0e-05 , 5.0e-06 
2025-01-11 00:15:04,281 - > saved mIoU
2025-01-11 00:31:10,065 - 4        0.235951   0.448056   0.482403   0.098141   0.913379   5.0e-05 , 5.0e-06 
2025-01-11 00:31:10,065 - > saved mIoU
2025-01-11 00:47:15,951 - 5        0.217072   0.467265   0.504084   0.193398   0.914532   5.0e-05 , 5.0e-06 
2025-01-11 00:47:15,952 - > saved mIoU
2025-01-11 01:03:20,670 - 6        0.199371   0.454568   0.520728   0.195128   0.914211   5.0e-05 , 5.0e-06 
2025-01-11 01:03:20,671 - > saved mIoU
2025-01-11 01:19:26,485 - 7        0.187614   0.459578   0.524229   0.209264   0.917520   5.0e-05 , 5.0e-06 
2025-01-11 01:19:26,485 - > saved mIoU
2025-01-11 01:35:32,819 - 8        0.178508   0.452237   0.528065   0.204655   0.916589   5.0e-05 , 5.0e-06 
2025-01-11 01:35:32,819 - > saved mIoU
2025-01-11 01:51:39,000 - 9        0.164583   0.443577   0.541788   0.221949   0.920832   5.0e-05 , 5.0e-06 
2025-01-11 01:51:39,001 - > saved mIoU
2025-01-11 02:07:44,763 - 10       0.156037   0.435378   0.551107   0.230238   0.925184   5.0e-05 , 5.0e-06 
2025-01-11 02:07:44,763 - > saved mIoU
2025-01-11 02:23:51,495 - 11       0.149338   0.421781   0.560750   0.244896   0.925591   5.0e-05 , 5.0e-06 
2025-01-11 02:23:51,495 - > saved mIoU
2025-01-11 02:39:57,892 - 12       0.142827   0.446935   0.551100   0.246380   0.923019   5.0e-05 , 5.0e-06 
2025-01-11 02:56:02,436 - 13       0.137471   0.432243   0.567414   0.248555   0.926621   5.0e-05 , 5.0e-06 
2025-01-11 02:56:02,436 - > saved mIoU
2025-01-11 03:12:07,973 - 14       0.133539   0.421794   0.570679   0.252072   0.926638   5.0e-05 , 5.0e-06 
2025-01-11 03:12:07,974 - > saved mIoU
2025-01-11 03:28:13,860 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-11 03:28:13,860 - 15       0.131276   0.401716   0.571279   0.260762   0.928632   5.0e-05 , 5.0e-06 
2025-01-11 03:28:13,861 - > saved mIoU
2025-01-11 03:44:19,056 - 16       0.127012   0.406727   0.583555   0.264742   0.925054   5.0e-05 , 5.0e-06 
2025-01-11 03:44:19,057 - > saved mIoU
2025-01-11 04:00:25,095 - 17       0.122221   0.389654   0.584712   0.264404   0.923791   5.0e-05 , 5.0e-06 
2025-01-11 04:00:25,095 - > saved mIoU
2025-01-11 04:16:30,603 - 18       0.118305   0.419622   0.585378   0.283801   0.923830   5.0e-05 , 5.0e-06 
2025-01-11 04:16:30,603 - > saved mIoU
2025-01-11 04:32:37,731 - 19       0.115727   0.405205   0.587808   0.255918   0.923444   5.0e-05 , 5.0e-06 
2025-01-11 04:32:37,731 - > saved mIoU
2025-01-11 04:48:43,580 - 20       0.112225   0.425763   0.584275   0.271950   0.919715   5.0e-05 , 5.0e-06 
2025-01-11 05:04:48,339 - 21       0.108396   0.403975   0.597943   0.288495   0.929670   5.0e-05 , 5.0e-06 
2025-01-11 05:04:48,340 - > saved mIoU
2025-01-11 05:20:53,758 - 22       0.107590   0.429474   0.582573   0.277391   0.922866   5.0e-05 , 5.0e-06 
2025-01-11 05:36:58,150 - 23       0.106305   0.384901   0.605181   0.295364   0.929677   5.0e-05 , 5.0e-06 
2025-01-11 05:36:58,150 - > saved mIoU
2025-01-11 05:53:03,952 - 24       0.103635   0.424708   0.584698   0.278192   0.927721   5.0e-05 , 5.0e-06 
2025-01-11 06:09:08,564 - 25       0.104705   0.399354   0.602065   0.310775   0.931051   5.0e-05 , 5.0e-06 
2025-01-11 06:25:12,985 - 26       0.099808   0.419527   0.592868   0.284456   0.928162   5.0e-05 , 5.0e-06 
2025-01-11 06:41:17,935 - 27       0.105013   0.396163   0.581033   0.265897   0.931039   5.0e-05 , 5.0e-06 
2025-01-11 06:57:22,772 - 28       0.102959   0.411301   0.598281   0.308357   0.927002   5.0e-05 , 5.0e-06 
2025-01-11 07:13:26,596 - 29       0.100746   0.400302   0.594530   0.309042   0.934266   5.0e-05 , 5.0e-06 
2025-01-11 07:29:30,993 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-11 07:29:30,994 - 30       0.096698   0.407617   0.612486   0.299180   0.931097   2.5e-05 , 2.5e-06 
2025-01-11 07:29:30,994 - > saved mIoU
2025-01-11 07:45:36,157 - 31       0.092702   0.412652   0.608320   0.294297   0.927726   2.5e-05 , 2.5e-06 
2025-01-11 08:01:42,104 - 32       0.091593   0.415824   0.611191   0.324762   0.928710   2.5e-05 , 2.5e-06 
2025-01-11 08:17:46,346 - 33       0.088878   0.413748   0.606476   0.305138   0.928210   2.5e-05 , 2.5e-06 
2025-01-11 08:33:51,783 - 34       0.088635   0.425613   0.602541   0.321571   0.929716   2.5e-05 , 2.5e-06 
2025-01-11 08:49:56,614 - 35       0.087239   0.410608   0.606163   0.323296   0.927175   2.5e-05 , 2.5e-06 
2025-01-11 09:06:02,381 - 36       0.086979   0.413026   0.602085   0.322535   0.925969   1.3e-05 , 1.3e-06 
2025-01-11 09:22:08,065 - 37       0.085097   0.412809   0.607500   0.313645   0.926574   1.3e-05 , 1.3e-06 
2025-01-11 09:38:13,635 - 38       0.084942   0.413106   0.608046   0.320407   0.929932   1.3e-05 , 1.3e-06 
2025-01-11 09:54:18,535 - 39       0.085321   0.400955   0.609928   0.301070   0.929939   1.3e-05 , 1.3e-06 
2025-01-11 10:10:23,517 - 40       0.083240   0.412480   0.609810   0.336888   0.929992   1.3e-05 , 1.3e-06 
2025-01-11 10:26:28,292 - 41       0.082645   0.410228   0.605702   0.309373   0.931547   1.3e-05 , 1.3e-06 
2025-01-11 10:42:33,574 - 42       0.082221   0.426117   0.607023   0.325917   0.932169   6.3e-06 , 6.3e-07 
2025-01-11 10:58:39,294 - 43       0.081810   0.420138   0.607163   0.329407   0.932334   6.3e-06 , 6.3e-07 
2025-01-11 11:14:44,290 - 44       0.081943   0.427550   0.605790   0.315866   0.931816   6.3e-06 , 6.3e-07 
2025-01-11 11:30:48,393 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-11 11:30:48,393 - 45       0.081447   0.429996   0.607152   0.329770   0.931261   6.3e-06 , 6.3e-07 
2025-01-11 11:46:51,586 - 46       0.081047   0.421743   0.606691   0.325231   0.930943   6.3e-06 , 6.3e-07 
2025-01-11 12:02:54,290 - 47       0.080607   0.416712   0.608807   0.329935   0.929309   6.3e-06 , 6.3e-07 
2025-01-11 12:18:56,311 - 48       0.080406   0.411902   0.610161   0.328194   0.930305   3.1e-06 , 3.1e-07 
2025-01-11 12:34:58,389 - 49       0.079985   0.414297   0.608530   0.329911   0.931269   3.1e-06 , 3.1e-07 
2025-01-11 12:34:58,390 - Fine del training

