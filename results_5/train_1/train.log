2025-01-08 21:01:54,295 - device          : cuda:1
2025-01-08 21:01:54,296 - model           : resnet101
2025-01-08 21:01:54,296 - mlp             : 1
2025-01-08 21:01:54,296 - vae             : False
2025-01-08 21:01:54,296 - epochs          : 50
2025-01-08 21:01:54,296 - batch           : 16
2025-01-08 21:01:54,296 - lr              : 0.0001
2025-01-08 21:01:54,296 - crop            : 512
2025-01-08 21:01:54,296 - optimizer       : adam
2025-01-08 21:01:54,296 - biases          : False
2025-01-08 21:01:54,296 - focal_loss      : 0
2025-01-08 21:01:54,296 - activation      : softmax
2025-01-08 21:01:54,296 - class_weights   : False
2025-01-08 21:01:54,296 - norm_weights    : False
2025-01-08 21:01:54,296 - p               : 0.2
2025-01-08 21:01:54,305 - Dataset Creati
2025-01-08 21:01:54,305 - Training images: 5125
2025-01-08 21:01:54,305 - Validation images: 1031

2025-01-08 21:01:54,305 - Dataloader Creati
2025-01-08 21:01:54,305 - Training batches: 320
2025-01-08 21:01:54,305 - Validation batches: 128

2025-01-08 21:01:58,026 - Model: deeplabv3plus_resnet101

2025-01-08 21:01:58,095 - Modello caricato correttamente su cuda:1

2025-01-08 21:01:58,099 - Inizio del training
2025-01-08 21:01:58,099 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-01-08 21:01:58,099 - loss:      CrossEntropyLoss()
2025-01-08 21:01:58,099 - epochs:    15
2025-01-08 21:01:58,099 - name:      weights_0

2025-01-08 21:08:28,079 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-08 21:08:28,080 - 0        0.981883   0.655944   0.269098   0.000000   0.820726   1.0e-04 , 1.0e-05 
2025-01-08 21:14:58,399 - 1        0.680191   0.601505   0.304969   0.000000   0.830475   1.0e-04 , 1.0e-05 
2025-01-08 21:14:58,399 - > saved mIoU
2025-01-08 21:21:30,069 - 2        0.617727   0.589284   0.329393   0.000000   0.841291   1.0e-04 , 1.0e-05 
2025-01-08 21:21:30,069 - > saved mIoU
2025-01-08 21:28:01,959 - 3        0.588130   0.576289   0.339779   0.000000   0.842355   1.0e-04 , 1.0e-05 
2025-01-08 21:28:01,959 - > saved mIoU
2025-01-08 21:34:32,810 - 4        0.553589   0.575906   0.343433   0.000000   0.845934   1.0e-04 , 1.0e-05 
2025-01-08 21:34:32,810 - > saved mIoU
2025-01-08 21:41:04,654 - 5        0.537221   0.556726   0.351185   0.000043   0.846324   1.0e-04 , 1.0e-05 
2025-01-08 21:41:04,654 - > saved mIoU
2025-01-08 21:47:35,690 - 6        0.519838   0.566429   0.355324   0.001351   0.850025   1.0e-04 , 1.0e-05 
2025-01-08 21:47:35,690 - > saved mIoU
2025-01-08 21:54:06,772 - 7        0.498839   0.567874   0.357667   0.005802   0.849679   1.0e-04 , 1.0e-05 
2025-01-08 21:54:06,773 - > saved mIoU
2025-01-08 22:00:38,323 - 8        0.489350   0.568665   0.366055   0.020740   0.851412   1.0e-04 , 1.0e-05 
2025-01-08 22:00:38,323 - > saved mIoU
2025-01-08 22:07:08,821 - 9        0.477341   0.567812   0.371688   0.028545   0.849564   1.0e-04 , 1.0e-05 
2025-01-08 22:07:08,821 - > saved mIoU
2025-01-08 22:13:40,234 - 10       0.462407   0.573259   0.378474   0.031224   0.853818   1.0e-04 , 1.0e-05 
2025-01-08 22:13:40,235 - > saved mIoU
2025-01-08 22:20:12,746 - 11       0.461475   0.583882   0.374189   0.031592   0.853095   1.0e-04 , 1.0e-05 
2025-01-08 22:26:43,545 - 12       0.447329   0.568403   0.388481   0.033997   0.854096   5.0e-05 , 5.0e-06 
2025-01-08 22:26:43,546 - > saved mIoU
2025-01-08 22:33:15,231 - 13       0.432135   0.569717   0.391515   0.035353   0.855164   5.0e-05 , 5.0e-06 
2025-01-08 22:33:15,231 - > saved mIoU
2025-01-08 22:39:45,346 - 14       0.428335   0.565747   0.394788   0.038078   0.853286   5.0e-05 , 5.0e-06 
2025-01-08 22:39:45,346 - > saved mIoU
2025-01-08 22:39:45,896 - Fine del training

2025-01-08 22:39:46,178 - 
Caricati i pesi in: /raid/homespace/piecestola/space/ML4CV/results/train_1/ckpts/weights_mIoU_0.pt

2025-01-08 22:39:46,179 - Inizio del training
2025-01-08 22:39:46,180 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 5e-05
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 5e-06
    maximize: False
    weight_decay: 0.0001
)
2025-01-08 22:39:46,180 - loss:      CrossEntropyLoss()
2025-01-08 22:39:46,180 - epochs:    50
2025-01-08 22:39:46,180 - name:      weights_1

2025-01-08 22:55:51,869 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-08 22:55:51,870 - 0        0.370602   0.458211   0.443066   0.065767   0.887063   5.0e-05 , 5.0e-06 
2025-01-08 23:11:58,664 - 1        0.287528   0.434724   0.479928   0.117826   0.904749   5.0e-05 , 5.0e-06 
2025-01-08 23:11:58,665 - > saved mIoU
2025-01-08 23:28:06,408 - 2        0.231290   0.404596   0.510671   0.167602   0.915974   5.0e-05 , 5.0e-06 
2025-01-08 23:28:06,409 - > saved mIoU
2025-01-08 23:44:13,393 - 3        0.206578   0.410456   0.516161   0.145502   0.916538   5.0e-05 , 5.0e-06 
2025-01-08 23:44:13,393 - > saved mIoU
2025-01-09 00:00:21,153 - 4        0.182782   0.414669   0.532074   0.168478   0.920842   5.0e-05 , 5.0e-06 
2025-01-09 00:00:21,153 - > saved mIoU
2025-01-09 00:16:29,544 - 5        0.165006   0.403446   0.538750   0.182451   0.923191   5.0e-05 , 5.0e-06 
2025-01-09 00:16:29,545 - > saved mIoU
2025-01-09 00:32:38,695 - 6        0.150710   0.401754   0.549596   0.191025   0.923836   5.0e-05 , 5.0e-06 
2025-01-09 00:32:38,695 - > saved mIoU
2025-01-09 00:48:46,738 - 7        0.139978   0.392143   0.552813   0.185668   0.920154   5.0e-05 , 5.0e-06 
2025-01-09 00:48:46,738 - > saved mIoU
2025-01-09 01:04:54,637 - 8        0.131948   0.391901   0.550899   0.198559   0.923692   5.0e-05 , 5.0e-06 
2025-01-09 01:21:00,676 - 9        0.124821   0.393342   0.564345   0.202097   0.926373   5.0e-05 , 5.0e-06 
2025-01-09 01:21:00,677 - > saved mIoU
2025-01-09 01:37:09,243 - 10       0.121036   0.413493   0.560402   0.202691   0.925807   5.0e-05 , 5.0e-06 
2025-01-09 01:53:14,100 - 11       0.115966   0.376203   0.574841   0.224680   0.931786   5.0e-05 , 5.0e-06 
2025-01-09 01:53:14,100 - > saved mIoU
2025-01-09 02:09:21,939 - 12       0.110957   0.389111   0.574540   0.271000   0.928544   5.0e-05 , 5.0e-06 
2025-01-09 02:25:29,028 - 13       0.108045   0.402577   0.571414   0.213732   0.928225   5.0e-05 , 5.0e-06 
2025-01-09 02:41:36,659 - 14       0.105407   0.372301   0.590609   0.215869   0.929283   5.0e-05 , 5.0e-06 
2025-01-09 02:41:36,660 - > saved mIoU
2025-01-09 02:57:43,374 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 02:57:43,374 - 15       0.102274   0.377531   0.586425   0.280244   0.930616   5.0e-05 , 5.0e-06 
2025-01-09 03:13:50,264 - 16       0.098057   0.381158   0.590495   0.239688   0.932524   5.0e-05 , 5.0e-06 
2025-01-09 03:29:56,347 - 17       0.095819   0.387076   0.592400   0.253858   0.933368   5.0e-05 , 5.0e-06 
2025-01-09 03:29:56,347 - > saved mIoU
2025-01-09 03:46:05,674 - 18       0.093796   0.376311   0.597463   0.207104   0.932253   5.0e-05 , 5.0e-06 
2025-01-09 03:46:05,674 - > saved mIoU
2025-01-09 04:02:13,210 - 19       0.092004   0.408381   0.581573   0.256695   0.933603   5.0e-05 , 5.0e-06 
2025-01-09 04:18:19,613 - 20       0.089343   0.391699   0.589509   0.206011   0.932222   5.0e-05 , 5.0e-06 
2025-01-09 04:34:26,775 - 21       0.087223   0.382765   0.605945   0.309310   0.933213   2.5e-05 , 2.5e-06 
2025-01-09 04:34:26,776 - > saved mIoU
2025-01-09 04:50:35,173 - 22       0.085074   0.391638   0.601761   0.274976   0.936337   2.5e-05 , 2.5e-06 
2025-01-09 05:06:41,254 - 23       0.082989   0.397307   0.602912   0.264453   0.934750   2.5e-05 , 2.5e-06 
2025-01-09 05:22:46,968 - 24       0.081485   0.396714   0.604282   0.233004   0.933868   2.5e-05 , 2.5e-06 
2025-01-09 05:38:53,462 - 25       0.080754   0.398629   0.603400   0.254356   0.933492   2.5e-05 , 2.5e-06 
2025-01-09 05:55:00,643 - 26       0.079411   0.383558   0.609576   0.267787   0.932029   2.5e-05 , 2.5e-06 
2025-01-09 05:55:00,643 - > saved mIoU
2025-01-09 06:11:09,509 - 27       0.079382   0.393446   0.613048   0.325154   0.931210   1.3e-05 , 1.3e-06 
2025-01-09 06:11:09,510 - > saved mIoU
2025-01-09 06:27:17,097 - 28       0.078003   0.384196   0.612124   0.301794   0.928444   1.3e-05 , 1.3e-06 
2025-01-09 06:43:22,377 - 29       0.076885   0.391652   0.610874   0.311890   0.931184   1.3e-05 , 1.3e-06 
2025-01-09 06:59:28,420 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 06:59:28,421 - 30       0.074817   0.397985   0.606633   0.306349   0.933705   1.3e-05 , 1.3e-06 
2025-01-09 07:15:34,596 - 31       0.074745   0.396942   0.611301   0.297787   0.934781   1.3e-05 , 1.3e-06 
2025-01-09 07:31:39,378 - 32       0.074114   0.395501   0.610629   0.294492   0.934773   1.3e-05 , 1.3e-06 
2025-01-09 07:47:45,224 - 33       0.073499   0.409271   0.612250   0.309488   0.934241   6.3e-06 , 6.3e-07 
2025-01-09 08:03:50,938 - 34       0.073654   0.399499   0.614907   0.304314   0.935164   6.3e-06 , 6.3e-07 
2025-01-09 08:03:50,939 - > saved mIoU
2025-01-09 08:19:58,950 - 35       0.072622   0.398864   0.619204   0.317832   0.934482   6.3e-06 , 6.3e-07 
2025-01-09 08:19:58,950 - > saved mIoU
2025-01-09 08:36:05,881 - 36       0.072239   0.398368   0.616908   0.306259   0.934981   6.3e-06 , 6.3e-07 
2025-01-09 08:52:11,854 - 37       0.071191   0.390605   0.621317   0.316824   0.936523   6.3e-06 , 6.3e-07 
2025-01-09 08:52:11,854 - > saved mIoU
2025-01-09 09:08:19,899 - 38       0.071595   0.397227   0.616298   0.311557   0.935715   6.3e-06 , 6.3e-07 
2025-01-09 09:24:25,509 - 39       0.071260   0.386983   0.619158   0.302839   0.934301   3.1e-06 , 3.1e-07 
2025-01-09 09:40:32,153 - 40       0.070706   0.394126   0.616220   0.297582   0.934416   3.1e-06 , 3.1e-07 
2025-01-09 09:56:37,499 - 41       0.070975   0.395102   0.618695   0.306940   0.934610   3.1e-06 , 3.1e-07 
2025-01-09 10:12:44,230 - 42       0.070693   0.395281   0.616770   0.305119   0.935578   3.1e-06 , 3.1e-07 
2025-01-09 10:28:50,020 - 43       0.070370   0.392662   0.617895   0.296447   0.934891   3.1e-06 , 3.1e-07 
2025-01-09 10:44:56,599 - 44       0.069786   0.391966   0.617950   0.297370   0.936447   3.1e-06 , 3.1e-07 
2025-01-09 11:01:04,100 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 11:01:04,100 - 45       0.070028   0.389628   0.616150   0.284396   0.935814   1.6e-06 , 1.6e-07 
2025-01-09 11:17:11,139 - 46       0.070172   0.388304   0.616749   0.284954   0.935493   1.6e-06 , 1.6e-07 
2025-01-09 11:33:16,413 - 47       0.070072   0.387919   0.617885   0.287864   0.935605   1.6e-06 , 1.6e-07 
2025-01-09 11:49:22,207 - 48       0.069807   0.389478   0.617773   0.287342   0.935096   1.6e-06 , 1.6e-07 
2025-01-09 12:05:26,332 - 49       0.069695   0.391943   0.616173   0.285085   0.935647   1.6e-06 , 1.6e-07 
2025-01-09 12:05:26,332 - Fine del training

