2025-01-08 21:01:58,616 - device          : cuda:3
2025-01-08 21:01:58,617 - model           : resnet101
2025-01-08 21:01:58,617 - mlp             : 1
2025-01-08 21:01:58,617 - vae             : False
2025-01-08 21:01:58,617 - epochs          : 50
2025-01-08 21:01:58,617 - batch           : 16
2025-01-08 21:01:58,617 - lr              : 0.0001
2025-01-08 21:01:58,617 - crop            : 512
2025-01-08 21:01:58,617 - optimizer       : adam
2025-01-08 21:01:58,617 - biases          : False
2025-01-08 21:01:58,617 - focal_loss      : 0
2025-01-08 21:01:58,617 - activation      : softmax
2025-01-08 21:01:58,617 - class_weights   : False
2025-01-08 21:01:58,617 - norm_weights    : False
2025-01-08 21:01:58,617 - p               : 0.3
2025-01-08 21:01:58,626 - Dataset Creati
2025-01-08 21:01:58,626 - Training images: 5125
2025-01-08 21:01:58,626 - Validation images: 1031

2025-01-08 21:01:58,626 - Dataloader Creati
2025-01-08 21:01:58,627 - Training batches: 320
2025-01-08 21:01:58,627 - Validation batches: 128

2025-01-08 21:02:02,277 - Model: deeplabv3plus_resnet101

2025-01-08 21:02:02,343 - Modello caricato correttamente su cuda:3

2025-01-08 21:02:02,346 - Inizio del training
2025-01-08 21:02:02,347 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-01-08 21:02:02,347 - loss:      CrossEntropyLoss()
2025-01-08 21:02:02,347 - epochs:    15
2025-01-08 21:02:02,347 - name:      weights_0

2025-01-08 21:08:24,188 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-08 21:08:24,188 - 0        0.994159   0.654326   0.268681   0.000000   0.820798   1.0e-04 , 1.0e-05 
2025-01-08 21:14:48,545 - 1        0.691013   0.599837   0.303261   0.000000   0.830322   1.0e-04 , 1.0e-05 
2025-01-08 21:14:48,545 - > saved mIoU
2025-01-08 21:21:12,700 - 2        0.628057   0.586983   0.327841   0.000000   0.841226   1.0e-04 , 1.0e-05 
2025-01-08 21:21:12,700 - > saved mIoU
2025-01-08 21:27:38,443 - 3        0.597949   0.573848   0.338667   0.000000   0.842303   1.0e-04 , 1.0e-05 
2025-01-08 21:27:38,443 - > saved mIoU
2025-01-08 21:34:02,612 - 4        0.563068   0.573560   0.342395   0.000000   0.845949   1.0e-04 , 1.0e-05 
2025-01-08 21:34:02,613 - > saved mIoU
2025-01-08 21:40:24,944 - 5        0.546408   0.554715   0.350220   0.000014   0.846445   1.0e-04 , 1.0e-05 
2025-01-08 21:40:24,945 - > saved mIoU
2025-01-08 21:46:47,625 - 6        0.528955   0.563697   0.353838   0.000289   0.849992   1.0e-04 , 1.0e-05 
2025-01-08 21:46:47,626 - > saved mIoU
2025-01-08 21:53:10,158 - 7        0.507370   0.564742   0.355929   0.002214   0.849594   1.0e-04 , 1.0e-05 
2025-01-08 21:53:10,159 - > saved mIoU
2025-01-08 21:59:32,665 - 8        0.497938   0.565138   0.362085   0.015688   0.851458   1.0e-04 , 1.0e-05 
2025-01-08 21:59:32,665 - > saved mIoU
2025-01-08 22:05:56,316 - 9        0.485617   0.563868   0.368438   0.024792   0.849464   1.0e-04 , 1.0e-05 
2025-01-08 22:05:56,316 - > saved mIoU
2025-01-08 22:12:19,862 - 10       0.470600   0.569018   0.373851   0.027419   0.853845   1.0e-04 , 1.0e-05 
2025-01-08 22:12:19,862 - > saved mIoU
2025-01-08 22:18:41,346 - 11       0.469521   0.579029   0.370960   0.028728   0.853302   1.0e-04 , 1.0e-05 
2025-01-08 22:25:02,373 - 12       0.455574   0.564058   0.384029   0.031142   0.854013   5.0e-05 , 5.0e-06 
2025-01-08 22:25:02,374 - > saved mIoU
2025-01-08 22:31:24,197 - 13       0.440199   0.565698   0.386892   0.032332   0.855161   5.0e-05 , 5.0e-06 
2025-01-08 22:31:24,197 - > saved mIoU
2025-01-08 22:37:45,627 - 14       0.436381   0.560927   0.390364   0.035162   0.853125   5.0e-05 , 5.0e-06 
2025-01-08 22:37:45,628 - > saved mIoU
2025-01-08 22:37:46,164 - Fine del training

2025-01-08 22:37:46,444 - 
Caricati i pesi in: /raid/homespace/piecestola/space/ML4CV/results/train_2/ckpts/weights_mIoU_0.pt

2025-01-08 22:37:46,445 - Inizio del training
2025-01-08 22:37:46,446 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 5e-05
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 5e-06
    maximize: False
    weight_decay: 0.0001
)
2025-01-08 22:37:46,446 - loss:      CrossEntropyLoss()
2025-01-08 22:37:46,446 - epochs:    50
2025-01-08 22:37:46,446 - name:      weights_1

2025-01-08 22:53:45,938 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-08 22:53:45,938 - 0        0.377256   0.454307   0.437492   0.063255   0.887757   5.0e-05 , 5.0e-06 
2025-01-08 23:09:44,527 - 1        0.293197   0.429365   0.473439   0.111080   0.904538   5.0e-05 , 5.0e-06 
2025-01-08 23:09:44,528 - > saved mIoU
2025-01-08 23:25:44,306 - 2        0.236362   0.400301   0.505916   0.177327   0.915421   5.0e-05 , 5.0e-06 
2025-01-08 23:25:44,307 - > saved mIoU
2025-01-08 23:41:44,243 - 3        0.211374   0.404427   0.514467   0.152925   0.917522   5.0e-05 , 5.0e-06 
2025-01-08 23:41:44,243 - > saved mIoU
2025-01-08 23:57:46,283 - 4        0.186859   0.409301   0.531011   0.183788   0.922293   5.0e-05 , 5.0e-06 
2025-01-08 23:57:46,283 - > saved mIoU
2025-01-09 00:13:49,395 - 5        0.168554   0.400145   0.538990   0.197277   0.923767   5.0e-05 , 5.0e-06 
2025-01-09 00:13:49,395 - > saved mIoU
2025-01-09 00:29:49,542 - 6        0.153882   0.394626   0.551841   0.206176   0.925321   5.0e-05 , 5.0e-06 
2025-01-09 00:29:49,542 - > saved mIoU
2025-01-09 00:45:50,286 - 7        0.142876   0.385516   0.555947   0.211670   0.921127   5.0e-05 , 5.0e-06 
2025-01-09 00:45:50,286 - > saved mIoU
2025-01-09 01:01:55,974 - 8        0.134632   0.385779   0.554980   0.201354   0.924433   5.0e-05 , 5.0e-06 
2025-01-09 01:17:57,023 - 9        0.127172   0.391240   0.564711   0.219704   0.926530   5.0e-05 , 5.0e-06 
2025-01-09 01:17:57,024 - > saved mIoU
2025-01-09 01:33:57,980 - 10       0.122840   0.406838   0.562290   0.213322   0.924244   5.0e-05 , 5.0e-06 
2025-01-09 01:49:56,553 - 11       0.118120   0.375446   0.575710   0.228084   0.931894   5.0e-05 , 5.0e-06 
2025-01-09 01:49:56,553 - > saved mIoU
2025-01-09 02:05:56,861 - 12       0.113319   0.383510   0.574766   0.266311   0.927999   5.0e-05 , 5.0e-06 
2025-01-09 02:21:54,758 - 13       0.110068   0.399082   0.574013   0.224089   0.925534   5.0e-05 , 5.0e-06 
2025-01-09 02:37:54,234 - 14       0.107320   0.371781   0.593040   0.224440   0.929260   5.0e-05 , 5.0e-06 
2025-01-09 02:37:54,234 - > saved mIoU
2025-01-09 02:53:57,054 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 02:53:57,055 - 15       0.104039   0.369874   0.588067   0.287762   0.930879   5.0e-05 , 5.0e-06 
2025-01-09 03:09:56,211 - 16       0.099752   0.378212   0.592996   0.248588   0.932612   5.0e-05 , 5.0e-06 
2025-01-09 03:25:55,593 - 17       0.097396   0.381202   0.594889   0.254356   0.933473   5.0e-05 , 5.0e-06 
2025-01-09 03:25:55,593 - > saved mIoU
2025-01-09 03:41:59,994 - 18       0.095857   0.374098   0.603706   0.233888   0.932310   5.0e-05 , 5.0e-06 
2025-01-09 03:41:59,995 - > saved mIoU
2025-01-09 03:58:01,181 - 19       0.093964   0.406182   0.582148   0.256908   0.932786   5.0e-05 , 5.0e-06 
2025-01-09 04:14:01,950 - 20       0.091295   0.382800   0.595292   0.232797   0.932266   5.0e-05 , 5.0e-06 
2025-01-09 04:30:05,797 - 21       0.088944   0.369436   0.609500   0.294466   0.932876   5.0e-05 , 5.0e-06 
2025-01-09 04:30:05,797 - > saved mIoU
2025-01-09 04:46:10,611 - 22       0.088075   0.408143   0.592491   0.284549   0.932606   5.0e-05 , 5.0e-06 
2025-01-09 05:02:13,647 - 23       0.086445   0.377263   0.612645   0.306394   0.937707   5.0e-05 , 5.0e-06 
2025-01-09 05:02:13,647 - > saved mIoU
2025-01-09 05:18:16,561 - 24       0.085313   0.381010   0.607128   0.251602   0.934317   5.0e-05 , 5.0e-06 
2025-01-09 05:34:17,854 - 25       0.084932   0.376632   0.612977   0.253517   0.936130   5.0e-05 , 5.0e-06 
2025-01-09 05:34:17,854 - > saved mIoU
2025-01-09 05:50:20,721 - 26       0.082588   0.374391   0.616970   0.306832   0.938116   5.0e-05 , 5.0e-06 
2025-01-09 05:50:20,722 - > saved mIoU
2025-01-09 06:06:24,565 - 27       0.081367   0.417011   0.602275   0.281411   0.932730   5.0e-05 , 5.0e-06 
2025-01-09 06:22:24,764 - 28       0.080934   0.412974   0.607713   0.288363   0.932982   2.5e-05 , 2.5e-06 
2025-01-09 06:38:24,122 - 29       0.077489   0.387110   0.622516   0.304720   0.929887   2.5e-05 , 2.5e-06 
2025-01-09 06:38:24,122 - > saved mIoU
2025-01-09 06:54:26,047 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 06:54:26,047 - 30       0.074724   0.395776   0.609137   0.301155   0.930015   2.5e-05 , 2.5e-06 
2025-01-09 07:10:26,986 - 31       0.074201   0.390221   0.627099   0.340296   0.931569   2.5e-05 , 2.5e-06 
2025-01-09 07:10:26,987 - > saved mIoU
2025-01-09 07:26:30,548 - 32       0.073183   0.389219   0.608132   0.275564   0.930879   2.5e-05 , 2.5e-06 
2025-01-09 07:42:32,557 - 33       0.072491   0.404044   0.620508   0.307278   0.933461   2.5e-05 , 2.5e-06 
2025-01-09 07:58:34,266 - 34       0.072957   0.414172   0.623187   0.330226   0.936732   1.3e-05 , 1.3e-06 
2025-01-09 08:14:34,945 - 35       0.070969   0.403435   0.632777   0.333852   0.936580   1.3e-05 , 1.3e-06 
2025-01-09 08:14:34,946 - > saved mIoU
2025-01-09 08:30:39,461 - 36       0.070089   0.408500   0.627849   0.338124   0.936236   1.3e-05 , 1.3e-06 
2025-01-09 08:46:40,923 - 37       0.068819   0.398001   0.631310   0.342672   0.936446   1.3e-05 , 1.3e-06 
2025-01-09 09:02:41,829 - 38       0.069123   0.406933   0.621530   0.341483   0.937047   1.3e-05 , 1.3e-06 
2025-01-09 09:18:45,020 - 39       0.068568   0.399596   0.632687   0.339078   0.935358   1.3e-05 , 1.3e-06 
2025-01-09 09:34:48,629 - 40       0.068023   0.411970   0.627628   0.337533   0.936920   6.3e-06 , 6.3e-07 
2025-01-09 09:50:52,508 - 41       0.067917   0.414624   0.630136   0.344265   0.937056   6.3e-06 , 6.3e-07 
2025-01-09 10:06:55,641 - 42       0.067503   0.414979   0.628886   0.342505   0.936986   6.3e-06 , 6.3e-07 
2025-01-09 10:22:57,301 - 43       0.067076   0.408613   0.630104   0.332484   0.935827   6.3e-06 , 6.3e-07 
2025-01-09 10:38:58,552 - 44       0.066432   0.405702   0.631808   0.337730   0.937931   6.3e-06 , 6.3e-07 
2025-01-09 10:54:59,254 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 10:54:59,255 - 45       0.066483   0.406202   0.629783   0.335392   0.936829   6.3e-06 , 6.3e-07 
2025-01-09 11:11:02,296 - 46       0.066703   0.398389   0.633356   0.335640   0.936250   3.1e-06 , 3.1e-07 
2025-01-09 11:11:02,296 - > saved mIoU
2025-01-09 11:27:05,662 - 47       0.066388   0.400193   0.633373   0.330619   0.936325   3.1e-06 , 3.1e-07 
2025-01-09 11:27:05,663 - > saved mIoU
2025-01-09 11:43:08,521 - 48       0.066017   0.401191   0.632826   0.326572   0.936176   3.1e-06 , 3.1e-07 
2025-01-09 11:59:11,473 - 49       0.065866   0.405317   0.629070   0.321660   0.937159   3.1e-06 , 3.1e-07 
2025-01-09 11:59:11,474 - Fine del training

