2025-01-08 21:02:12,267 - device          : cuda:6
2025-01-08 21:02:12,267 - model           : resnet101
2025-01-08 21:02:12,267 - mlp             : 2
2025-01-08 21:02:12,267 - vae             : False
2025-01-08 21:02:12,267 - epochs          : 50
2025-01-08 21:02:12,267 - batch           : 16
2025-01-08 21:02:12,267 - lr              : 0.0001
2025-01-08 21:02:12,267 - crop            : 512
2025-01-08 21:02:12,267 - optimizer       : adam
2025-01-08 21:02:12,268 - biases          : False
2025-01-08 21:02:12,268 - focal_loss      : 0
2025-01-08 21:02:12,268 - activation      : softmax
2025-01-08 21:02:12,268 - class_weights   : False
2025-01-08 21:02:12,268 - norm_weights    : False
2025-01-08 21:02:12,268 - p               : 0.4
2025-01-08 21:02:12,276 - Dataset Creati
2025-01-08 21:02:12,277 - Training images: 5125
2025-01-08 21:02:12,277 - Validation images: 1031

2025-01-08 21:02:12,277 - Dataloader Creati
2025-01-08 21:02:12,277 - Training batches: 320
2025-01-08 21:02:12,277 - Validation batches: 128

2025-01-08 21:02:16,276 - Model: deeplabv3plus_resnet101

2025-01-08 21:02:16,344 - Modello caricato correttamente su cuda:6

2025-01-08 21:02:16,347 - Inizio del training
2025-01-08 21:02:16,348 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-01-08 21:02:16,348 - loss:      CrossEntropyLoss()
2025-01-08 21:02:16,348 - epochs:    15
2025-01-08 21:02:16,348 - name:      weights_0

2025-01-08 21:08:48,690 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-08 21:08:48,690 - 0        1.168751   0.647141   0.248846   0.000000   0.814208   1.0e-04 , 1.0e-05 
2025-01-08 21:15:19,953 - 1        0.806103   0.606471   0.271641   0.000000   0.824976   1.0e-04 , 1.0e-05 
2025-01-08 21:15:19,953 - > saved mIoU
2025-01-08 21:21:50,733 - 2        0.715484   0.587725   0.290430   0.000000   0.834098   1.0e-04 , 1.0e-05 
2025-01-08 21:21:50,733 - > saved mIoU
2025-01-08 21:28:23,015 - 3        0.675514   0.568838   0.311015   0.000000   0.837776   1.0e-04 , 1.0e-05 
2025-01-08 21:28:23,016 - > saved mIoU
2025-01-08 21:34:54,745 - 4        0.640665   0.563722   0.325342   0.000000   0.836490   1.0e-04 , 1.0e-05 
2025-01-08 21:34:54,745 - > saved mIoU
2025-01-08 21:41:26,849 - 5        0.622800   0.561096   0.340965   0.000000   0.845215   1.0e-04 , 1.0e-05 
2025-01-08 21:41:26,849 - > saved mIoU
2025-01-08 21:47:59,931 - 6        0.590256   0.559043   0.343620   0.000000   0.843649   1.0e-04 , 1.0e-05 
2025-01-08 21:47:59,931 - > saved mIoU
2025-01-08 21:54:31,776 - 7        0.583174   0.554515   0.351563   0.000000   0.849522   1.0e-04 , 1.0e-05 
2025-01-08 21:54:31,777 - > saved mIoU
2025-01-08 22:01:03,476 - 8        0.558909   0.551337   0.355285   0.000000   0.850103   1.0e-04 , 1.0e-05 
2025-01-08 22:01:03,476 - > saved mIoU
2025-01-08 22:07:34,214 - 9        0.543867   0.560491   0.357964   0.000000   0.845515   1.0e-04 , 1.0e-05 
2025-01-08 22:07:34,214 - > saved mIoU
2025-01-08 22:14:05,301 - 10       0.531858   0.546278   0.359663   0.000000   0.846481   1.0e-04 , 1.0e-05 
2025-01-08 22:14:05,302 - > saved mIoU
2025-01-08 22:20:36,155 - 11       0.522710   0.552425   0.362636   0.000000   0.848396   1.0e-04 , 1.0e-05 
2025-01-08 22:20:36,156 - > saved mIoU
2025-01-08 22:27:06,985 - 12       0.511238   0.551726   0.365113   0.000000   0.845683   1.0e-04 , 1.0e-05 
2025-01-08 22:27:06,985 - > saved mIoU
2025-01-08 22:33:38,039 - 13       0.501319   0.546867   0.366723   0.000000   0.851105   1.0e-04 , 1.0e-05 
2025-01-08 22:33:38,039 - > saved mIoU
2025-01-08 22:40:09,220 - 14       0.491327   0.548026   0.370957   0.000000   0.845498   1.0e-04 , 1.0e-05 
2025-01-08 22:40:09,221 - > saved mIoU
2025-01-08 22:40:09,787 - Fine del training

2025-01-08 22:40:10,071 - 
Caricati i pesi in: /raid/homespace/piecestola/space/ML4CV/results/train_5/ckpts/weights_mIoU_0.pt

2025-01-08 22:40:10,072 - Inizio del training
2025-01-08 22:40:10,072 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-01-08 22:40:10,073 - loss:      CrossEntropyLoss()
2025-01-08 22:40:10,073 - epochs:    50
2025-01-08 22:40:10,073 - name:      weights_1

2025-01-08 22:56:15,310 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-08 22:56:15,310 - 0        0.431477   0.475889   0.410269   0.000000   0.902429   1.0e-04 , 1.0e-05 
2025-01-08 23:12:20,924 - 1        0.323124   0.388639   0.446442   0.000000   0.908802   1.0e-04 , 1.0e-05 
2025-01-08 23:12:20,924 - > saved mIoU
2025-01-08 23:28:28,440 - 2        0.264895   0.377600   0.467110   0.000000   0.923255   1.0e-04 , 1.0e-05 
2025-01-08 23:28:28,441 - > saved mIoU
2025-01-08 23:44:35,198 - 3        0.220071   0.371528   0.480563   0.000000   0.920091   1.0e-04 , 1.0e-05 
2025-01-08 23:44:35,198 - > saved mIoU
2025-01-09 00:00:40,522 - 4        0.192707   0.348957   0.506768   0.000000   0.927804   1.0e-04 , 1.0e-05 
2025-01-09 00:00:40,523 - > saved mIoU
2025-01-09 00:16:48,123 - 5        0.173697   0.358856   0.511114   0.000000   0.924184   1.0e-04 , 1.0e-05 
2025-01-09 00:16:48,124 - > saved mIoU
2025-01-09 00:32:56,093 - 6        0.157980   0.352689   0.531388   0.000000   0.930055   1.0e-04 , 1.0e-05 
2025-01-09 00:32:56,094 - > saved mIoU
2025-01-09 00:49:04,995 - 7        0.146869   0.378793   0.530967   0.000000   0.931645   1.0e-04 , 1.0e-05 
2025-01-09 01:05:12,419 - 8        0.137398   0.359986   0.528170   0.000000   0.929080   1.0e-04 , 1.0e-05 
2025-01-09 01:21:18,227 - 9        0.131668   0.381299   0.534417   0.000000   0.931470   1.0e-04 , 1.0e-05 
2025-01-09 01:21:18,227 - > saved mIoU
2025-01-09 01:37:24,969 - 10       0.124605   0.381696   0.546852   0.000000   0.931333   1.0e-04 , 1.0e-05 
2025-01-09 01:37:24,970 - > saved mIoU
2025-01-09 01:53:32,704 - 11       0.121750   0.369204   0.548861   0.000000   0.932879   5.0e-05 , 5.0e-06 
2025-01-09 01:53:32,705 - > saved mIoU
2025-01-09 02:09:38,830 - 12       0.121078   0.373790   0.556165   0.000000   0.932618   5.0e-05 , 5.0e-06 
2025-01-09 02:09:38,831 - > saved mIoU
2025-01-09 02:25:46,622 - 13       0.113083   0.369712   0.556416   0.000000   0.931647   5.0e-05 , 5.0e-06 
2025-01-09 02:25:46,623 - > saved mIoU
2025-01-09 02:41:53,236 - 14       0.107934   0.376943   0.560492   0.000000   0.926549   5.0e-05 , 5.0e-06 
2025-01-09 02:41:53,236 - > saved mIoU
2025-01-09 02:57:58,315 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 02:57:58,316 - 15       0.105601   0.358389   0.565297   0.000935   0.929977   5.0e-05 , 5.0e-06 
2025-01-09 02:57:58,316 - > saved mIoU
2025-01-09 03:14:04,750 - 16       0.102880   0.377474   0.572161   0.109702   0.932626   5.0e-05 , 5.0e-06 
2025-01-09 03:14:04,751 - > saved mIoU
2025-01-09 03:30:10,805 - 17       0.102380   0.397833   0.573171   0.026188   0.929806   2.5e-05 , 2.5e-06 
2025-01-09 03:30:10,805 - > saved mIoU
2025-01-09 03:46:19,072 - 18       0.099749   0.392403   0.579796   0.128004   0.927362   2.5e-05 , 2.5e-06 
2025-01-09 03:46:19,073 - > saved mIoU
2025-01-09 04:02:25,299 - 19       0.097773   0.396735   0.573523   0.032623   0.930676   2.5e-05 , 2.5e-06 
2025-01-09 04:18:30,739 - 20       0.095667   0.395364   0.586699   0.216035   0.928989   2.5e-05 , 2.5e-06 
2025-01-09 04:18:30,739 - > saved mIoU
2025-01-09 04:34:39,453 - 21       0.094440   0.392175   0.589484   0.271440   0.930774   2.5e-05 , 2.5e-06 
2025-01-09 04:34:39,453 - > saved mIoU
2025-01-09 04:50:48,503 - 22       0.092112   0.402301   0.605312   0.324234   0.936365   2.5e-05 , 2.5e-06 
2025-01-09 04:50:48,504 - > saved mIoU
2025-01-09 05:06:58,921 - 23       0.092504   0.398239   0.601275   0.310496   0.934726   1.3e-05 , 1.3e-06 
2025-01-09 05:23:06,307 - 24       0.090723   0.404622   0.608626   0.318015   0.936845   1.3e-05 , 1.3e-06 
2025-01-09 05:23:06,308 - > saved mIoU
2025-01-09 05:39:15,474 - 25       0.089468   0.410499   0.611215   0.311936   0.934975   1.3e-05 , 1.3e-06 
2025-01-09 05:39:15,474 - > saved mIoU
2025-01-09 05:55:23,002 - 26       0.089364   0.409476   0.615521   0.319431   0.935013   1.3e-05 , 1.3e-06 
2025-01-09 05:55:23,003 - > saved mIoU
2025-01-09 06:11:28,901 - 27       0.087614   0.405273   0.614797   0.312419   0.933149   1.3e-05 , 1.3e-06 
2025-01-09 06:27:36,010 - 28       0.087827   0.413852   0.615677   0.322328   0.935003   1.3e-05 , 1.3e-06 
2025-01-09 06:27:36,011 - > saved mIoU
2025-01-09 06:43:43,001 - 29       0.087372   0.409608   0.617844   0.315801   0.935779   6.3e-06 , 6.3e-07 
2025-01-09 06:43:43,001 - > saved mIoU
2025-01-09 06:59:50,544 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 06:59:50,544 - 30       0.085948   0.418495   0.617261   0.320633   0.934712   6.3e-06 , 6.3e-07 
2025-01-09 07:15:57,051 - 31       0.085197   0.410811   0.616538   0.322105   0.934128   6.3e-06 , 6.3e-07 
2025-01-09 07:32:01,234 - 32       0.085227   0.410037   0.622391   0.319463   0.934417   6.3e-06 , 6.3e-07 
2025-01-09 07:32:01,234 - > saved mIoU
2025-01-09 07:48:08,506 - 33       0.084182   0.408361   0.620006   0.322496   0.935456   6.3e-06 , 6.3e-07 
2025-01-09 08:04:13,520 - 34       0.084496   0.412565   0.619460   0.316692   0.933679   6.3e-06 , 6.3e-07 
2025-01-09 08:20:18,653 - 35       0.083736   0.409918   0.622684   0.320049   0.936025   3.1e-06 , 3.1e-07 
2025-01-09 08:20:18,654 - > saved mIoU
2025-01-09 08:36:27,202 - 36       0.083975   0.409728   0.622762   0.323019   0.935363   3.1e-06 , 3.1e-07 
2025-01-09 08:36:27,202 - > saved mIoU
2025-01-09 08:52:35,592 - 37       0.082915   0.410754   0.623581   0.324598   0.936369   3.1e-06 , 3.1e-07 
2025-01-09 08:52:35,593 - > saved mIoU
2025-01-09 09:08:43,744 - 38       0.082896   0.411120   0.623380   0.321553   0.935389   3.1e-06 , 3.1e-07 
2025-01-09 09:24:50,442 - 39       0.082531   0.408037   0.625900   0.321894   0.935646   3.1e-06 , 3.1e-07 
2025-01-09 09:24:50,443 - > saved mIoU
2025-01-09 09:40:59,508 - 40       0.082567   0.414818   0.621844   0.328175   0.935085   3.1e-06 , 3.1e-07 
2025-01-09 09:57:06,453 - 41       0.081992   0.410836   0.623369   0.327609   0.935447   1.6e-06 , 1.6e-07 
2025-01-09 10:13:14,563 - 42       0.082644   0.409883   0.624406   0.326346   0.934974   1.6e-06 , 1.6e-07 
2025-01-09 10:29:19,502 - 43       0.081212   0.409665   0.623957   0.324550   0.935266   1.6e-06 , 1.6e-07 
2025-01-09 10:45:25,899 - 44       0.082228   0.412227   0.622373   0.324603   0.935488   1.6e-06 , 1.6e-07 
2025-01-09 11:01:32,478 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-09 11:01:32,478 - 45       0.081622   0.411436   0.623310   0.323676   0.935080   1.6e-06 , 1.6e-07 
2025-01-09 11:17:37,967 - 46       0.081262   0.410577   0.623874   0.325220   0.935557   1.6e-06 , 1.6e-07 
2025-01-09 11:33:44,511 - 47       0.080708   0.413332   0.622742   0.323396   0.934650   7.8e-07 , 7.8e-08 
2025-01-09 11:49:50,319 - 48       0.081321   0.410475   0.623374   0.325307   0.935887   7.8e-07 , 7.8e-08 
2025-01-09 12:05:58,073 - 49       0.081108   0.411585   0.622485   0.326506   0.935416   7.8e-07 , 7.8e-08 
2025-01-09 12:05:58,074 - Fine del training

