2025-01-10 20:28:37,618 - device          : cuda:1
2025-01-10 20:28:37,618 - model           : resnet101
2025-01-10 20:28:37,618 - mlp             : 0
2025-01-10 20:28:37,618 - vae             : False
2025-01-10 20:28:37,618 - epochs          : 50
2025-01-10 20:28:37,618 - batch           : 16
2025-01-10 20:28:37,618 - lr              : 0.0001
2025-01-10 20:28:37,618 - crop            : 512
2025-01-10 20:28:37,618 - optimizer       : adam
2025-01-10 20:28:37,618 - biases          : False
2025-01-10 20:28:37,618 - focal_loss      : 0
2025-01-10 20:28:37,618 - activation      : softmax
2025-01-10 20:28:37,618 - class_weights   : False
2025-01-10 20:28:37,618 - norm_weights    : False
2025-01-10 20:28:37,619 - p               : 0.3
2025-01-10 20:28:37,626 - Dataset Creati
2025-01-10 20:28:37,626 - Training images: 5125
2025-01-10 20:28:37,626 - Validation images: 1031

2025-01-10 20:28:37,627 - Dataloader Creati
2025-01-10 20:28:37,627 - Training batches: 320
2025-01-10 20:28:37,627 - Validation batches: 128

2025-01-10 20:28:41,128 - Model: deeplabv3plus_resnet101

2025-01-10 20:28:41,191 - Modello caricato correttamente su cuda:1

2025-01-10 20:28:41,194 - Inizio del training
2025-01-10 20:28:41,194 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-01-10 20:28:41,194 - loss:      CrossEntropyLoss()
2025-01-10 20:28:41,194 - epochs:    15
2025-01-10 20:28:41,195 - name:      weights_0

2025-01-10 20:34:51,937 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-01-10 20:34:51,937 - 0        1.123199   0.819506   0.263782   0.000000   0.776092   1.0e-04 , 1.0e-05 
2025-01-10 20:41:01,801 - 1        0.790579   0.732915   0.293665   0.000000   0.802890   1.0e-04 , 1.0e-05 
2025-01-10 20:41:01,801 - > saved mIoU
2025-01-10 20:47:13,445 - 2        0.717304   0.686283   0.308195   0.000000   0.811709   1.0e-04 , 1.0e-05 
2025-01-10 20:47:13,445 - > saved mIoU
2025-01-10 20:53:23,755 - 3        0.670647   0.676339   0.312241   0.000000   0.815214   1.0e-04 , 1.0e-05 
2025-01-10 20:53:23,755 - > saved mIoU
2025-01-10 20:59:33,955 - 4        0.652712   0.682948   0.321057   0.000000   0.813592   1.0e-04 , 1.0e-05 
2025-01-10 20:59:33,955 - > saved mIoU
2025-01-10 21:05:44,178 - 5        0.632524   0.702684   0.321431   0.000000   0.800484   1.0e-04 , 1.0e-05 
2025-01-10 21:05:44,178 - > saved mIoU
2025-01-10 21:11:56,557 - 6        0.612186   0.671911   0.331103   0.000000   0.816379   1.0e-04 , 1.0e-05 
2025-01-10 21:11:56,558 - > saved mIoU
2025-01-10 21:18:08,759 - 7        0.595708   0.668409   0.332956   0.000000   0.821703   1.0e-04 , 1.0e-05 
2025-01-10 21:18:08,760 - > saved mIoU
