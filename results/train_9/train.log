2025-02-01 17:03:48,159 - device          : cuda:1
2025-02-01 17:03:48,159 - model           : resnet101
2025-02-01 17:03:48,160 - mlp             : 0
2025-02-01 17:03:48,160 - vae             : False
2025-02-01 17:03:48,160 - epochs          : 50
2025-02-01 17:03:48,160 - batch           : 16
2025-02-01 17:03:48,160 - lr              : 0.0001
2025-02-01 17:03:48,160 - crop            : 512
2025-02-01 17:03:48,160 - optimizer       : adam
2025-02-01 17:03:48,160 - biases          : False
2025-02-01 17:03:48,160 - focal_loss      : 0
2025-02-01 17:03:48,160 - activation      : softmax
2025-02-01 17:03:48,160 - class_weights   : False
2025-02-01 17:03:48,160 - norm_weights    : False
2025-02-01 17:03:48,160 - p               : 0.3
2025-02-01 17:03:48,160 - entropy         : 0.1
2025-02-01 17:03:48,168 - Dataset Creati
2025-02-01 17:03:48,168 - Training images: 5125
2025-02-01 17:03:48,168 - Validation images: 1031

2025-02-01 17:03:48,169 - Dataloader Creati
2025-02-01 17:03:48,169 - Training batches: 320
2025-02-01 17:03:48,169 - Validation batches: 128

2025-02-01 17:03:52,060 - Model: deeplabv3plus_resnet101

2025-02-01 17:03:52,129 - Modello caricato correttamente su cuda:1

2025-02-01 17:03:52,132 - Inizio del training
2025-02-01 17:03:52,132 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-02-01 17:03:52,133 - loss:      CE_EntropyMinimization(
  (ce_loss): CrossEntropyLoss()
)
2025-02-01 17:03:52,133 - epochs:    15
2025-02-01 17:03:52,133 - name:      weights_0

2025-02-01 17:10:09,555 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-02-01 17:10:09,555 - 0        1.246823   0.896982   0.257599   0.000000   0.777264   1.0e-04 , 1.0e-05 
2025-02-01 17:16:25,021 - 1        0.873259   0.801493   0.288305   0.000000   0.803065   1.0e-04 , 1.0e-05 
2025-02-01 17:16:25,022 - > saved mIoU
2025-02-01 17:22:39,466 - 2        0.791460   0.748760   0.303875   0.000000   0.812313   1.0e-04 , 1.0e-05 
2025-02-01 17:22:39,467 - > saved mIoU
2025-02-01 17:28:53,388 - 3        0.740034   0.738309   0.308457   0.000000   0.815635   1.0e-04 , 1.0e-05 
2025-02-01 17:28:53,389 - > saved mIoU
2025-02-01 17:35:07,925 - 4        0.720323   0.744923   0.318122   0.000000   0.814534   1.0e-04 , 1.0e-05 
2025-02-01 17:35:07,925 - > saved mIoU
2025-02-01 17:41:21,812 - 5        0.698016   0.765313   0.318910   0.000000   0.802607   1.0e-04 , 1.0e-05 
2025-02-01 17:41:21,812 - > saved mIoU
2025-02-01 17:47:36,637 - 6        0.675797   0.732509   0.328211   0.000000   0.817566   1.0e-04 , 1.0e-05 
2025-02-01 17:47:36,638 - > saved mIoU
2025-02-01 17:53:50,666 - 7        0.657759   0.728630   0.329869   0.000000   0.822675   1.0e-04 , 1.0e-05 
2025-02-01 17:53:50,666 - > saved mIoU
2025-02-01 18:00:04,758 - 8        0.645393   0.717907   0.338428   0.000000   0.823855   1.0e-04 , 1.0e-05 
2025-02-01 18:00:04,758 - > saved mIoU
2025-02-01 18:06:19,686 - 9        0.631323   0.731316   0.337175   0.000000   0.824917   1.0e-04 , 1.0e-05 
2025-02-01 18:12:33,805 - 10       0.622983   0.741301   0.344528   0.000000   0.818438   1.0e-04 , 1.0e-05 
2025-02-01 18:12:33,806 - > saved mIoU
2025-02-01 18:18:49,183 - 11       0.614135   0.751013   0.345802   0.000000   0.816220   1.0e-04 , 1.0e-05 
2025-02-01 18:18:49,184 - > saved mIoU
2025-02-01 18:25:04,106 - 12       0.610059   0.735371   0.350210   0.000000   0.817129   1.0e-04 , 1.0e-05 
2025-02-01 18:25:04,106 - > saved mIoU
2025-02-01 18:31:18,395 - 13       0.593262   0.740674   0.349253   0.000000   0.823712   1.0e-04 , 1.0e-05 
2025-02-01 18:37:33,175 - 14       0.597919   0.756557   0.350165   0.001033   0.815786   1.0e-04 , 1.0e-05 
2025-02-01 18:37:33,176 - Fine del training

2025-02-01 18:37:33,459 - 
Caricati i pesi in: /raid/homespace/piecestola/space/ML4CV/results/train_9/ckpts/weights_mIoU_0.pt

2025-02-01 18:37:33,460 - Inizio del training
2025-02-01 18:37:33,460 - optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.0001
    maximize: False
    weight_decay: 0.0001

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 1e-05
    maximize: False
    weight_decay: 0.0001
)
2025-02-01 18:37:33,460 - loss:      CE_EntropyMinimization(
  (ce_loss): CrossEntropyLoss()
)
2025-02-01 18:37:33,460 - epochs:    50
2025-02-01 18:37:33,460 - name:      weights_1

2025-02-01 18:53:24,859 - Epoch    Loss       Val Loss   mIoU       minIoU     maxIoU     LR      
2025-02-01 18:53:24,860 - 0        0.494175   0.519489   0.422562   0.000000   0.884932   1.0e-04 , 1.0e-05 
2025-02-01 19:09:16,178 - 1        0.357861   0.474014   0.450053   0.000000   0.912714   1.0e-04 , 1.0e-05 
2025-02-01 19:09:16,179 - > saved mIoU
2025-02-01 19:25:11,401 - 2        0.284650   0.459495   0.470229   0.000000   0.906433   1.0e-04 , 1.0e-05 
2025-02-01 19:25:11,401 - > saved mIoU
2025-02-01 19:41:05,011 - 3        0.237883   0.449472   0.498869   0.124671   0.911588   1.0e-04 , 1.0e-05 
2025-02-01 19:41:05,012 - > saved mIoU
2025-02-01 19:56:54,550 - 4        0.207323   0.425499   0.513077   0.131049   0.914192   1.0e-04 , 1.0e-05 
2025-02-01 19:56:54,550 - > saved mIoU
2025-02-01 20:12:45,181 - 5        0.189198   0.436272   0.538170   0.221438   0.920914   1.0e-04 , 1.0e-05 
2025-02-01 20:12:45,182 - > saved mIoU
2025-02-01 20:28:37,249 - 6        0.172043   0.427721   0.550021   0.219606   0.924299   1.0e-04 , 1.0e-05 
2025-02-01 20:28:37,249 - > saved mIoU
2025-02-01 20:44:25,673 - 7        0.161586   0.436493   0.543129   0.223332   0.926726   1.0e-04 , 1.0e-05 
